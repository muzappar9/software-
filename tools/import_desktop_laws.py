#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¼å…¥æ¡Œé¢æ³•å¾‹æ–‡ä»¶åˆ°LawPackæ•°æ®åº“çš„å·¥å…·
ä» C:\Users\26581\Desktop\æ³•å¾‹èµ„æ–™ å¯¼å…¥8ä¸ªæ³•å¾‹æ–‡ä»¶
"""

import os
import sqlite3
import re
from pathlib import Path
import docx
import PyPDF2

def extract_text_from_docx(file_path):
    """ä»DOCXæ–‡ä»¶æå–æ–‡æœ¬"""
    try:
        doc = docx.Document(file_path)
        text = []
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():
                text.append(paragraph.text.strip())
        return '\n'.join(text)
    except Exception as e:
        print(f"Error reading DOCX {file_path}: {e}")
        return ""

def extract_text_from_pdf(file_path):
    """ä»PDFæ–‡ä»¶æå–æ–‡æœ¬"""
    try:
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = []
            for page in reader.pages:
                text.append(page.extract_text())
        return '\n'.join(text)
    except Exception as e:
        print(f"Error reading PDF {file_path}: {e}")
        return ""

def split_into_chunks(text, law_name):
    """å°†æ³•å¾‹æ–‡æœ¬åˆ†å‰²ä¸ºæ¡æ–‡å—"""
    # åŒ¹é…æ³•æ¡æ ¼å¼ï¼šç¬¬Xæ¡ã€ç¬¬Xç« ã€ç¬¬XèŠ‚ç­‰
    patterns = [
        r'(?=ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ã€‡0-9]+\s*æ¡)',
        r'(?=ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ã€‡0-9]+\s*ç« )',
        r'(?=ç¬¬\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶ã€‡0-9]+\s*èŠ‚)',
    ]
    
    chunks = []
    for pattern in patterns:
        parts = re.split(pattern, text)
        if len(parts) > 1:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„åˆ†å‰²æ¨¡å¼
            for i, part in enumerate(parts[1:], 1):  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºç™½éƒ¨åˆ†
                if part.strip():
                    chunks.append({
                        'content': part.strip()[:2000],  # é™åˆ¶é•¿åº¦
                        'article_id': f"{law_name}_chunk_{i}",
                        'law_name': law_name
                    })
            break
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¡æ–‡åˆ†å‰²ï¼ŒæŒ‰æ®µè½åˆ†å‰²
    if not chunks:
        paragraphs = text.split('\n')
        for i, para in enumerate(paragraphs):
            if para.strip() and len(para.strip()) > 20:
                chunks.append({
                    'content': para.strip()[:2000],
                    'article_id': f"{law_name}_para_{i}",
                    'law_name': law_name
                })
    
    return chunks

def import_law_files():
    """å¯¼å…¥æ¡Œé¢ä¸Šçš„8ä¸ªæ³•å¾‹æ–‡ä»¶"""
    
    # æ¡Œé¢æ³•å¾‹æ–‡ä»¶è·¯å¾„
    desktop_law_path = r"C:\Users\26581\Desktop\æ³•å¾‹èµ„æ–™"
    
    # æ³•å¾‹æ–‡ä»¶åˆ—è¡¨
    law_files = [
        ("ä¸­åäººæ°‘å…±å’Œå›½é“è·¯äº¤é€šå®‰å…¨æ³•.docx", "é“è·¯äº¤é€šå®‰å…¨æ³•"),
        ("ä¸­åäººæ°‘æ°‘æ³•å…¸.docx", "æ°‘æ³•å…¸"),
        ("ä¸­å›½äººæ°‘å…±å’Œå›½åŠ³åŠ¨åˆåŒæ³•.docx", "åŠ³åŠ¨åˆåŒæ³•"),
        ("ä¸­å›½äººæ°‘å…±å’Œå›½åŠ³åŠ¨æ³•.docx", "åŠ³åŠ¨æ³•"),
        ("ä¸­å›½äººæ°‘å…±å’Œå›½æ°‘äº‹è¯‰è®¼æ³•.pdf", "æ°‘äº‹è¯‰è®¼æ³•"),
        ("å·¥ä¼¤ä¿é™©æ¡ä¾‹.docx", "å·¥ä¼¤ä¿é™©æ¡ä¾‹"),
        ("æ¶ˆè´¹è€…æƒç›Šä¿æŠ¤æ³•.docx", "æ¶ˆè´¹è€…æƒç›Šä¿æŠ¤æ³•"),
        ("è¡Œæ”¿å¤„ç½šæ³•.docx", "è¡Œæ”¿å¤„ç½šæ³•")
    ]
    
    # æ•°æ®åº“è·¯å¾„
    db_path = "assets/lawpack.db"
    
    # è¿æ¥æ•°æ®åº“
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    total_chunks = 0
    total_articles = 0
    
    for filename, law_name in law_files:
        file_path = os.path.join(desktop_law_path, filename)
        
        if not os.path.exists(file_path):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            continue
            
        print(f"ğŸ“– æ­£åœ¨å¤„ç†: {law_name}")
        
        # æå–æ–‡æœ¬
        if filename.endswith('.docx'):
            text = extract_text_from_docx(file_path)
        elif filename.endswith('.pdf'):
            text = extract_text_from_pdf(file_path)
        else:
            continue
            
        if not text:
            print(f"  âš ï¸ æ— æ³•æå–æ–‡æœ¬: {filename}")
            continue
            
        # åˆ†å‰²ä¸ºæ¡æ–‡å—
        chunks = split_into_chunks(text, law_name)
        
        if not chunks:
            print(f"  âš ï¸ æ— æ³•åˆ†å‰²æ¡æ–‡: {filename}")
            continue
        
        # æ’å…¥articlesè¡¨
        try:
            cursor.execute("""
                INSERT OR REPLACE INTO articles (id, title, content, source, lang)
                VALUES (?, ?, ?, ?, 'zh')
            """, (law_name, law_name, text[:5000], filename))
            total_articles += 1
        except Exception as e:
            print(f"  âŒ æ’å…¥æ–‡ç« å¤±è´¥: {e}")
            continue
        
        # æ’å…¥chunksè¡¨
        chunk_count = 0
        for chunk in chunks:
            try:
                cursor.execute("""
                    INSERT OR REPLACE INTO chunks (id, article_id, content, embedding, lang)
                    VALUES (?, ?, ?, NULL, 'zh')
                """, (chunk['article_id'], law_name, chunk['content']))
                chunk_count += 1
            except Exception as e:
                print(f"  âŒ æ’å…¥æ¡æ–‡å—å¤±è´¥: {e}")
                continue
        
        total_chunks += chunk_count
        print(f"  âœ… å®Œæˆ: {chunk_count} ä¸ªæ¡æ–‡å—")
    
    # æäº¤æ›´æ”¹
    conn.commit()
    conn.close()
    
    print(f"\nğŸ‰ å¯¼å…¥å®Œæˆ!")
    print(f"ğŸ“š æ€»æ–‡ç« æ•°: {total_articles}")
    print(f"ğŸ“„ æ€»æ¡æ–‡å—: {total_chunks}")
    print(f"ğŸ’¾ æ•°æ®åº“: {db_path}")

if __name__ == "__main__":
    import_law_files()