#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ³•å¾‹æ–‡æ¡£è§£æå™¨ - å°†Word/PDFæ³•å¾‹æ–‡ä»¶è½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼
Legal Document Parser - Convert Word/PDF legal files to database format
"""

import os
import sys
import sqlite3
import json
from pathlib import Path
from typing import List, Dict, Tuple
import re

# å°è¯•å¯¼å…¥æ–‡æ¡£è§£æåº“
try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    print("âš ï¸  python-docx not available. Install with: pip install python-docx")

try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    print("âš ï¸  PyPDF2 not available. Install with: pip install PyPDF2")

class LegalDocumentParser:
    """æ³•å¾‹æ–‡æ¡£è§£æå™¨"""
    
    def __init__(self, db_path: str):
        """åˆå§‹åŒ–è§£æå™¨
        
        Args:
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        """
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºæ³•å¾‹æ–‡æ¡£è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS legal_documents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                filename TEXT NOT NULL,
                file_type TEXT NOT NULL,
                content TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºæ³•å¾‹æ¡æ–‡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS legal_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_id INTEGER,
                chapter TEXT,
                article_number TEXT,
                article_title TEXT,
                content TEXT NOT NULL,
                keywords TEXT,
                FOREIGN KEY (document_id) REFERENCES legal_documents (id)
            )
        ''')
        
        # åˆ›å»ºå…¨æ–‡æœç´¢è¡¨
        cursor.execute('''
            CREATE VIRTUAL TABLE IF NOT EXISTS legal_search USING fts5(
                title, content, keywords,
                content=legal_articles,
                content_rowid=id
            )
        ''')
        
        conn.commit()
        conn.close()
        print("âœ… æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")
    
    def extract_text_from_docx(self, file_path: str) -> str:
        """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
        if not DOCX_AVAILABLE:
            return "ERROR: python-docx not available"
        
        try:
            doc = Document(file_path)
            text = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text.append(paragraph.text.strip())
            return '\n'.join(text)
        except Exception as e:
            return f"ERROR: Failed to extract from {file_path}: {str(e)}"
    
    def extract_text_from_pdf(self, file_path: str) -> str:
        """ä»PDFæ–‡æ¡£æå–æ–‡æœ¬"""
        if not PDF_AVAILABLE:
            return "ERROR: PyPDF2 not available"
        
        try:
            with open(file_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = []
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text.strip():
                        text.append(page_text.strip())
                return '\n'.join(text)
        except Exception as e:
            return f"ERROR: Failed to extract from {file_path}: {str(e)}"
    
    def parse_legal_articles(self, content: str, doc_title: str) -> List[Dict]:
        """è§£ææ³•å¾‹æ¡æ–‡"""
        articles = []
        
        # åŒ¹é…æ³•å¾‹æ¡æ–‡çš„æ­£åˆ™è¡¨è¾¾å¼
        patterns = [
            r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+)æ¡\s*(.+?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+æ¡|$)',
            r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+)ç« \s*(.+?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+ç« |ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+æ¡|$)',
            r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+)ã€\s*(.+?)(?=[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+ã€|$)',
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content, re.DOTALL | re.MULTILINE)
            for match in matches:
                article_num = match.group(1)
                article_content = match.group(2).strip()
                
                if len(article_content) > 10:  # è¿‡æ»¤è¿‡çŸ­çš„å†…å®¹
                    # æå–å…³é”®è¯
                    keywords = self.extract_keywords(article_content)
                    
                    articles.append({
                        'article_number': article_num,
                        'content': article_content,
                        'keywords': ', '.join(keywords),
                        'chapter': self.detect_chapter(article_content)
                    })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æ„åŒ–æ¡æ–‡ï¼Œå°†æ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªæ¡ç›®
        if not articles:
            keywords = self.extract_keywords(content)
            articles.append({
                'article_number': 'å…¨æ–‡',
                'content': content,
                'keywords': ', '.join(keywords),
                'chapter': doc_title
            })
        
        return articles
    
    def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # æ³•å¾‹ç›¸å…³å…³é”®è¯
        legal_keywords = [
            'åˆåŒ', 'åè®®', 'è¿çº¦', 'èµ”å¿', 'è´£ä»»', 'ä¹‰åŠ¡', 'æƒåˆ©', 'æ³•å¾‹',
            'å©šå§»', 'ç¦»å©š', 'è´¢äº§', 'æŠšå…»', 'ç»§æ‰¿', 'é—äº§',
            'åŠ³åŠ¨', 'å·¥ä¼¤', 'ç¤¾ä¿', 'å·¥èµ„', 'è¾èŒ', 'è§£é›‡',
            'äº¤é€š', 'äº‹æ•…', 'ä¿é™©', 'ç†èµ”', 'é©¾é©¶', 'è¿ç« ',
            'æ¶ˆè´¹', 'æƒç›Š', 'æ¬ºè¯ˆ', 'é€€è´§', 'è´¨é‡', 'æœåŠ¡',
            'è¡Œæ”¿', 'å¤„ç½š', 'ç¨‹åº', 'ç”³è¯‰', 'å¤è®®', 'è¯‰è®¼',
            'æ°‘äº‹', 'åˆ‘äº‹', 'è¯æ®', 'å®¡åˆ¤', 'æ‰§è¡Œ', 'ä»²è£'
        ]
        
        found_keywords = []
        for keyword in legal_keywords:
            if keyword in text:
                found_keywords.append(keyword)
        
        return found_keywords[:10]  # é™åˆ¶å…³é”®è¯æ•°é‡
    
    def detect_chapter(self, content: str) -> str:
        """æ£€æµ‹ç« èŠ‚ä¿¡æ¯"""
        chapter_patterns = [
            r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+)ç¼–',
            r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+)ç« ',
            r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+)èŠ‚',
        ]
        
        for pattern in chapter_patterns:
            match = re.search(pattern, content)
            if match:
                return f"ç¬¬{match.group(1)}ç« "
        
        return "é€šç”¨æ¡æ¬¾"
    
    def process_document(self, file_path: str) -> bool:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        file_path = Path(file_path)
        
        if not file_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
        
        print(f"ğŸ“„ æ­£åœ¨å¤„ç†: {file_path.name}")
        
        # æå–æ–‡æœ¬å†…å®¹
        if file_path.suffix.lower() == '.docx':
            content = self.extract_text_from_docx(str(file_path))
        elif file_path.suffix.lower() == '.pdf':
            content = self.extract_text_from_pdf(str(file_path))
        else:
            print(f"âš ï¸  ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
            return False
        
        if content.startswith("ERROR:"):
            print(f"âŒ æ–‡æ¡£è§£æå¤±è´¥: {content}")
            return False
        
        # è·å–æ–‡æ¡£æ ‡é¢˜
        title = file_path.stem
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ’å…¥æ–‡æ¡£è®°å½•
        cursor.execute('''
            INSERT INTO legal_documents (title, filename, file_type, content)
            VALUES (?, ?, ?, ?)
        ''', (title, file_path.name, file_path.suffix, content))
        
        document_id = cursor.lastrowid
        
        # è§£æå¹¶æ’å…¥æ³•å¾‹æ¡æ–‡
        articles = self.parse_legal_articles(content, title)
        
        for article in articles:
            cursor.execute('''
                INSERT INTO legal_articles 
                (document_id, chapter, article_number, content, keywords)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                document_id,
                article['chapter'],
                article['article_number'],
                article['content'],
                article['keywords']
            ))
            
            # æ’å…¥å…¨æ–‡æœç´¢è¡¨
            cursor.execute('''
                INSERT INTO legal_search (title, content, keywords)
                VALUES (?, ?, ?)
            ''', (title, article['content'], article['keywords']))
        
        conn.commit()
        conn.close()
        
        print(f"âœ… å·²å¤„ç† {title}: {len(articles)} ä¸ªæ¡æ–‡")
        return True
    
    def process_directory(self, directory_path: str):
        """å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£"""
        directory = Path(directory_path)
        
        if not directory.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
            return
        
        print(f"ğŸ“ å¼€å§‹å¤„ç†ç›®å½•: {directory}")
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        supported_extensions = ['.docx', '.pdf']
        
        processed_count = 0
        failed_count = 0
        
        for file_path in directory.iterdir():
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                if self.process_document(file_path):
                    processed_count += 1
                else:
                    failed_count += 1
        
        print(f"\nğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
        print(f"âœ… æˆåŠŸå¤„ç†: {processed_count} ä¸ªæ–‡ä»¶")
        print(f"âŒ å¤„ç†å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
        
        # æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡
        self.show_database_stats()
    
    def show_database_stats(self):
        """æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ–‡æ¡£ç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) FROM legal_documents")
        doc_count = cursor.fetchone()[0]
        
        # æ¡æ–‡ç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) FROM legal_articles")
        article_count = cursor.fetchone()[0]
        
        # æŒ‰æ–‡æ¡£ç»Ÿè®¡æ¡æ–‡æ•°é‡
        cursor.execute('''
            SELECT d.title, COUNT(a.id) as article_count
            FROM legal_documents d
            LEFT JOIN legal_articles a ON d.id = a.document_id
            GROUP BY d.id, d.title
            ORDER BY article_count DESC
        ''')
        
        doc_stats = cursor.fetchall()
        
        conn.close()
        
        print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
        print(f"ğŸ“„ æ€»æ–‡æ¡£æ•°: {doc_count}")
        print(f"ğŸ“ æ€»æ¡æ–‡æ•°: {article_count}")
        print(f"\nğŸ“‹ å„æ–‡æ¡£æ¡æ–‡ç»Ÿè®¡:")
        
        for title, count in doc_stats:
            print(f"  â€¢ {title}: {count} æ¡")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ³•å¾‹æ–‡æ¡£è§£æå™¨å¯åŠ¨")
    
    # è®¾ç½®è·¯å¾„
    current_dir = Path.cwd()
    legal_docs_dir = current_dir / "assets" / "legal-data" / "laws"
    db_path = current_dir / "assets" / "lawpack.db"
    
    print(f"ğŸ“ æ³•å¾‹æ–‡æ¡£ç›®å½•: {legal_docs_dir}")
    print(f"ğŸ—ƒï¸  æ•°æ®åº“è·¯å¾„: {db_path}")
    
    # æ£€æŸ¥ä¾èµ–
    missing_deps = []
    if not DOCX_AVAILABLE:
        missing_deps.append("python-docx")
    if not PDF_AVAILABLE:
        missing_deps.append("PyPDF2")
    
    if missing_deps:
        print(f"\nâš ï¸  ç¼ºå°‘ä¾èµ–åº“: {', '.join(missing_deps)}")
        print("è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print(f"pip install {' '.join(missing_deps)}")
        return False
    
    # åˆ›å»ºè§£æå™¨å¹¶å¤„ç†æ–‡æ¡£
    parser = LegalDocumentParser(str(db_path))
    parser.process_directory(str(legal_docs_dir))
    
    print("\nğŸ‰ æ³•å¾‹æ–‡æ¡£è§£æå®Œæˆï¼")
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)